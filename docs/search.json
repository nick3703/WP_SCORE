[
  {
    "objectID": "by-statsds-topic.html",
    "href": "by-statsds-topic.html",
    "title": "Modules By Topic",
    "section": "",
    "text": "BlackJack Logistic Regression\n\n\n\n\n\n\nLogistic Regression\n\n\nBinary Data\n\n\n\nAn Introduction to Logistic Regression Using BlackJack\n\n\n\n\n\nJul 1, 2024\n\n\nAndrew Tran, Nicholas Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDoes Footedness Impact Free-Kick Accuracy?\n\n\n\n\n\n\nT-test\n\n\nExploratory Analysis\n\n\n\nDoes Footedness Impact Free-Kick Accuracy?\n\n\n\n\n\nSep 11, 2024\n\n\nBridget Ge, Claire Tsay\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Goals in Lacrosse\n\n\n\n\n\n\nLogistic Regression\n\n\nDecision Trees\n\n\nGini index\n\n\n\nAn Introduction to Expected Goals Using Lacrosse\n\n\n\n\n\nJul 1, 2024\n\n\nRyan Sellew, Andrew Lee\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Goals in Soccer\n\n\n\n\n\n\nLogistic Regression\n\n\nFeature Engineering\n\n\nUnder Sampling\n\n\n\nAn Introduction to Expected Goals Using Soccer\n\n\n\n\n\nJul 1, 2024\n\n\nColman Kim, Andrew Lee\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST Robotics Competition - Winning Chances\n\n\n\n\n\n\nBrier score\n\n\nprediction assessment\n\n\n\nEvaluating the predicted winning probabilities against the actual outcomes.\n\n\n\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIndoor Obstacle Course Test\n\n\n\n\n\n\nCollinearity\n\n\nMultiple Regression\n\n\n\nLearning about collinearity using IOCT Data\n\n\n\n\n\nJun 6, 2024\n\n\nNicholas Clark\n\n\n\n\n\n\n\n\n\n\n\n\nOrdinal Logistic Regression Basketball Salary Prediction\n\n\n\n\n\n\nOrdinal Regression\n\n\n\nAn Introduction to Ordinal Regression Using Basketball\n\n\n\n\n\nJul 1, 2024\n\n\nJohn Beggs, Skyler Chauff\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Allstars\n\n\n\n\n\n\nF1 Score\n\n\nConfusion Matrix\n\n\nModel Assessment\n\n\n\nModel Assessment through TPR/FPR, PPV/NPV, and F1 Score\n\n\n\n\n\nInvalid Date\n\n\nGarrett Salisbury\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Modules By Topic"
    ]
  },
  {
    "objectID": "soccer/Footedness/index.html",
    "href": "soccer/Footedness/index.html",
    "title": "Does Footedness Impact Free-Kick Accuracy?",
    "section": "",
    "text": "Solutions to this module are available here\nHere is the data associated with this module",
    "crumbs": [
      "Home",
      "Soccer",
      "Does Footedness Impact Free-Kick Accuracy?"
    ]
  },
  {
    "objectID": "soccer/Footedness/index.html#learning-goals",
    "href": "soccer/Footedness/index.html#learning-goals",
    "title": "Does Footedness Impact Free-Kick Accuracy?",
    "section": "1. Learning Goals",
    "text": "1. Learning Goals\nA student who successfully completed this module should have the ability to: \\(\\\\\\)\n\nApply the six steps of the statistical investigation method to comparing two groups on a quantitative response.\nCalculate the five-number summary (quartiles) and create histograms and box plots to explore the data from two groups with a quantitative response variable.\nDevelop a null and alternative hypothesis for a research question for comparing two means.\nAssess the statistical significance of the observed difference between two groups.\nApply the 3S Strategy to assess whether two sample means differ enough to conclude that there is a genuine difference in the population means or long-run means of a process.\nUse the 2SD method to estimate a confidence interval for the difference in two means.\nDetermine the strength of evidence using the theory-based approach (two-sample t-test) for comparing two means.",
    "crumbs": [
      "Home",
      "Soccer",
      "Does Footedness Impact Free-Kick Accuracy?"
    ]
  },
  {
    "objectID": "soccer/Footedness/index.html#introduction",
    "href": "soccer/Footedness/index.html#introduction",
    "title": "Does Footedness Impact Free-Kick Accuracy?",
    "section": "2. Introduction",
    "text": "2. Introduction\nSoccer, also known as football in other countries, is one of the most popular sports in Europe. In soccer, a player can either be right foot dominant or left foot dominant, and this can lead to differences in how players play. Left footed players are rare in professional soccer, and as such, other players may not be able to play against them as effectively. For example, left-footed shots and passes may be harder to block or intercept. As a result, left-footed players are often very sought after by teams. But does this belief have any statistical basis?\nThere are many ways to investigate this question, but the way this module compares the two groups is through a comparison of their free-kick accuracies. This measure was chosen because it was the variable that was most likely to be unaffected by other outside variables such as the player’s skill, speed, or other attributes, making it a relatively clear measurement of both kicking ability and the opposing team’s ability to block the free kick.\nIn order to answer this question, we will use data collected from soccer games all over Europe to determine each player’s preferred foot, group the players by their preferred foot, and calculate the mean free-kick accuracy score for each group. A quick side note regarding the free-kick accuracy score, this is not the percentage of free kicks made by the player, but a score from 0-99 assigned to a player based on their current performance in free-kicks.\nKnowing the calculated difference, we can use statistical analysis to determine if the difference in free-kick accuracy score between right-footed and left-footed players is more likely due to a difference between the two groups or just due to chance. In statistics, such an analysis is known as a two-sample t-test for a difference in means. This module will guide you through exploring the data, developing hypotheses, and determining the strength of evidence provided by the data.",
    "crumbs": [
      "Home",
      "Soccer",
      "Does Footedness Impact Free-Kick Accuracy?"
    ]
  },
  {
    "objectID": "soccer/Footedness/index.html#data",
    "href": "soccer/Footedness/index.html#data",
    "title": "Does Footedness Impact Free-Kick Accuracy?",
    "section": "3. Data",
    "text": "3. Data\nThe data for this project comes from the Kaggle Database titled “European Soccer Database: 25k+ matches, players & teams attributes for European Professional Football” uploaded by Hugo Mathien. The data for this dataset comes from multiple sources, including http://football-data.mx-api.enetscores.com/ (includes scores, lineup, team formation and events), http://www.football-data.co.uk/ (betting odds), and http://sofifa.com/ (player and team attributes).\nThe data includes a total of seven tables: Country, League, Match, Player, Player_Attributes, Team, and Team_Attributes, and these seven tables have a total of 199 columns. According to the description, the database contains data from 2008 to 2016 on more than 25,000 matches, 10,000 players and their attributes, 11 European countries and their lead championships, team line ups, betting odds, and detailed match events (goal types, possession, corner, cross, fouls, cards, etc.)\nThe code for loading the SQLite database and selecting the data of interest will be included in the appendix. This code loads the seven tables from the SQLite database into a separate data frame for each table. Once we have the data frames, we can narrow it down to the data that we actually need in order to conduct the analysis. Through looking at the data in each table, we can determine that the only table that applies to the question we are trying to answer is the Player_Attributes table. The code in the appendix writes the data in this table to a csv file, which is read using the code shown below.\n\nPlayer_Attributes &lt;- read_csv(\"https://raw.githubusercontent.com/nick3703/WP_SCORE/main/soccer/Footedness/Player_Attributes.csv\")\n\nNow that we know what table we want to focus on, we can take a closer look and select possible variables of interest. In addition, we can create some summary statistics and display them in a well-organized table. The code below shows how to filter out missing data, select certain columns of interest, and show summary statistics.\n\n#Data frame with attributes\nselect_player_attributes &lt;- Player_Attributes %&gt;%\n  drop_na() %&gt;%\n  select(preferred_foot, ball_control, free_kick_accuracy, overall_rating)\n\n#Table to show summary of data frame\nselect_player_attributes %&gt;%\n  group_by(preferred_foot) %&gt;%\n  summarize(mean_BC = mean(ball_control), \n            mean_FCA = mean(free_kick_accuracy), \n            mean_OR = mean(overall_rating), \n            n = n()) %&gt;%\n  kable(col.names = c(\"Preferred Foot\", \"Mean Ball Control Score\", \n                      \"Mean Free Kick Accuracy\", \"Mean Rating\", \"Number of Observations\"))\n\n\n\n\n\n\n\n\n\n\n\nPreferred Foot\nMean Ball Control Score\nMean Free Kick Accuracy\nMean Rating\nNumber of Observations\n\n\n\n\nleft\n65.65477\n53.52865\n68.82293\n23923\n\n\nright\n63.03961\n48.27246\n68.63414\n69702\n\n\n\n\n\nAs shown by the summary statistics, there do seem to be score differences between left-footed and right-footed players, with the Mean Free Kick Accuracy showing the greatest difference. This indicates to us that we are on the right track and should continue with our statistical analysis.",
    "crumbs": [
      "Home",
      "Soccer",
      "Does Footedness Impact Free-Kick Accuracy?"
    ]
  },
  {
    "objectID": "soccer/Footedness/index.html#methodsinstructional-content",
    "href": "soccer/Footedness/index.html#methodsinstructional-content",
    "title": "Does Footedness Impact Free-Kick Accuracy?",
    "section": "4. Methods/Instructional Content",
    "text": "4. Methods/Instructional Content\nIn order to draw inferences from the data, we will conduct a theory-based test to compare two means, also known as a two-sample t-test. A two-sample t-test calculates the standardized t-statistic, similar to the standardized z-statistic. However, while the z-statistic is used to compare differences in proportions, the t-statistic is used to compare differences in sample means. Therefore, a standardized t-statistic is suitable for this situation. T-statistic can be calculated by:\n\\[T = \\frac{\\text{mean}_1 - {\\text{mean}_2}}{SE(\\text{statistic})}\\]\n\\[SE = \\sqrt{\\frac{{s_1^2}}{{n_1}} - \\frac{{s_2^2}}{{n_2}}}\\]\nThe standardized statistic equation has the difference in sample means, also known as the statistics, as the numerator. The denominator is the standard error of the statistics. As the equation demonstrates, the t-statistic is the ratio of the mean difference to the standard error of the statistics. In other words, z-statistics are measurements of how far the mean of one group is from the mean of the second group, with the sign of the z-statistics indicating the direction of one group compared to the other. The larger the numerical value of the z-statistics indicates the larger the difference between the two means. For example, a z-statistic of 1 indicates that the mean of one group is either 1 standard deviation above or below the other group. Therefore, the larger the z-statistics the smaller the P-value, as smaller P-values indicate that it is more likely to reject the null hypothesis and suggests a difference between two means exists. In conclusion, the larger the z-statistics is, the stronger the evidence of difference in population means.\nHowever, for the theory-based approach to be valid, we must consider the two validity conditions. The first validity condition is that there must be at least 20 observations in each group without any strong skewness in the distributions. Secondly, the data are distributed symmetrically in both groups.\nThe two instructional content we picked were Introduction to Statistical Investigations, 2nd Edition, and Intermediate Statistical Investigations, 1st Edition.\\(\\\\\\) Scholarly reference 1: Introduction to Statistical Investigations (Chapter 8: Comparing more than two proportions) \\(\\\\\\) This chapter provides theoretical knowledge of the key components of our module as the chapter includes all basic information. We used this reference to refresh our knowledge of the process of comparing multiple proportions. It also includes information on generalization and causation.\\(\\\\\\) Scholarly reference 2: Intermediate Statistical Investigations (Section 6.1 Comparing Proportions) \\(\\\\\\) This scholarly reference provides a detailed explanation of the different methods to compare proportions, including the two-sample t-test. This source was helpful to the process of developing the model as it provides multiple examples of various cases of statistical investigations involving categorical datasets. By reading through the examples, we were able to develop the module by following the general question/exercise format as the examples, as it provides a very well-developed flow to guide readers through a problem. \\(\\\\\\)",
    "crumbs": [
      "Home",
      "Soccer",
      "Does Footedness Impact Free-Kick Accuracy?"
    ]
  },
  {
    "objectID": "soccer/Footedness/index.html#exercisesactivities",
    "href": "soccer/Footedness/index.html#exercisesactivities",
    "title": "Does Footedness Impact Free-Kick Accuracy?",
    "section": "5. Exercises/Activities",
    "text": "5. Exercises/Activities\nThe following are the exercises related to the module. These questions will help guide you through the six steps of statistical investigation in relation to our data and research question.\n\nThe first step to any statistical analysis is to ask a research question. In this case, we are trying to investigate a difference in means free-kick accuracy score between right-footed and left-footed players. What do you think is a possible research question that can be used for this?\n\nThe first two parts to this step include identifying the observational units in this study and determining the null and alternative hypotheses. An observational unit is what is actually being observed in the study. In this case, it would be a European Soccer Player. As for the Hypotheses, there are two that need to be made for purposes of statistical analysis. The first is the Null Hypothesis, which states that there is no real statistical evidence for anything and the second is the Alternative Hypothesis, which says that the data provides evidence for some sort of conclusion. In this case, we need to craft the hypotheses for our question regarding a difference in means. We chose to conduct a two-tailed test to see if there exists any sort of difference in means. The two null hypotheses we came up with for this study are stated below in both words and symbols. For reference, \\(\\mu_r\\) represents the mean free kick accuracy score for right footed players and \\(\\mu_l\\) represents the mean free kick accuracy score for left footed players.\nNull Hypothesis: There is no difference in mean free-kick accuracy score between left-footed and right-footed European Football players. \\[\\text{H}_0 : \\mu_r = \\mu_l \\]\nAlternative Hypothesis: There is a difference in mean free-kick accuracy score between left-footed and right-footed European Football players. \\[\\text{H}_a : \\mu_r \\neq \\mu_l \\] As for the data, we explained in the earlier data section how it was obtained, and we can use this data to conduct our statistical analysis.\n\nIn the Data section, we took a look at the data and selected the variables that we needed in order to conduct the statistical analysis, so now all we need to do is do some data exploration and visualization. One way to do this for comparing two means is to create the five-number summary and corresponding box-plot. What is a five-number summary?\n\n#Five number summary of each of the two populations\nselect_player_attributes %&gt;%\n  group_by(preferred_foot) %&gt;%\n  summarize(Minimum = min(free_kick_accuracy),\n            LowerQuartile = quantile(prob =.25, free_kick_accuracy),\n            Median = median(free_kick_accuracy),\n            UpperQuartile = quantile(prob=.75, free_kick_accuracy),\n            Maximum = max(free_kick_accuracy))\n\n#Boxplot to illustrate the five-number summary\nselect_player_attributes %&gt;%\n  ggplot(aes(x = free_kick_accuracy, \n             y = preferred_foot)) +\n  geom_boxplot()+\n  labs(y = \"Preferred Foot\", \n       x = \"Free Kick Accuracy\", \n       title = \"Free Kick Accuracy vs Preferred Foot\")\n\nCompute the five-number summary. What does the five-number summary and box plot show about the distribution of free-kick accuracy score between the two groups? Does it show evidence of a significant difference?\n\n#Histogram to visualize data\nselect_player_attributes %&gt;%\n  ggplot(aes(x=free_kick_accuracy)) + \n  geom_histogram() +\n  facet_grid(preferred_foot~.)\n\n\n\n\n\n\n\n\nWhat does the distribution of the histogram show? How does this add to the information shown by the boxplot?\nFrom the two visualizations we created, there does seem to be a difference between the mean free-kick accuracy score between the two groups, which means that it is probably worth investigating further using a statistical test.\n\nIn order to make conclusions beyond what is shown in the data, we must conduct a t-statistic test of the difference in means between the two groups and determine if our observed difference is statistically significant.\nTo conduct the two-sample t-test for a difference in means, we first need to calculate summarized statistics for each group, and using these observed statistics, we can calculate the standardized statistics needed in order to perform the statistical analysis.\n\n#Calculate Summarized Statistics\nselect_player_attributes %&gt;%\n  group_by(preferred_foot) %&gt;%\n  summarise(xbar = mean(free_kick_accuracy),\n            s = sd(free_kick_accuracy),\n            n = n())\n\n# A tibble: 2 × 4\n  preferred_foot  xbar     s     n\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 left            53.5  17.4 23923\n2 right           48.3  17.7 69702\n\n\nThere are a few standardized statistics we need for the two-sample t-test. xbar (\\(\\bar{x}\\)) is the observed statistic for each group, n is the sample size of each group, and s is the observed standard deviation of each group. Using the s value from each group, we can create a value that accounts for the differing sample size and standard deviations of each group, and this sd value is what we use to determine the value of the t-statistic for our analysis. This is known as the 2SD method.\n\n#Calculate Standardized Statistics\nxbar_left = 53.291\nxbar_right = 48.131\ns_left = 17.325\ns_right = 17.796\nn_left = 44107\nn_right = 136247\nsd = sqrt(s_left^2/n_left+s_right^2/n_right)\nnull = 0\nstatistic = xbar_left-xbar_right\nt = (statistic-null)/sd\nt\n\n[1] 54.00372\n\n\nWhat is our calculated t-statistic? What does this t-statistic represent and what kind of evidence does it provide for our hypotheses?\nNow that we have the t-statistic calculated, we can use it to determine the p-value, or the probability that the observed difference in means between the two groups was caused by chance alone. In order to do this, we use R’s pt function with n-2 degrees of freedom, as there are two groups being compared. In addition to this, we need to multiply whatever value we get from the pt function by two, as we are trying to check if there is any difference between the two means, meaning that it doesn’t matter which one is higher or which one is lower than the other, just that the two are different. When we do this, it is called a two-tailed test, as we are adding up the probabilities from both “tails” of the distribution.\n\n#Calculate P-Value with n-2 degrees of freedom, two-tailed test\nn = n_left+n_right\npvalue = 2*pt(t,n-2, lower.tail = FALSE)\npvalue\n\n[1] 0\n\n\nWhat is the p-value? When the p-value is compared to the significance level of 0.01, what does it show?\nFinally, we’ll calculate a confidence interval to determine what we think the actual difference between the two means is. Because we are using a 0.01 significance level, we will calculate the confidence interval with 99% confidence.\n\n#calculate Confidence interval at 99% confidence\nmultiplier = qt(.995,n-2)\nse = sd\nCI = c(statistic - multiplier*se, statistic + multiplier*se)\nCI\n\n[1] 4.91388 5.40612\n\n\nWhat does the 99% level of confidence represent?\nWe actually conducted this test the hard way, finding our own summary statistics for the data, but R actually has a function that will perform the t-test for you and display the t-statistic, p-value, and confidence interval. In fact, this whole test could have performed in just one line of code!\n\nt.test(free_kick_accuracy ~ preferred_foot, data = select_player_attributes)\n\n\n    Welch Two Sample t-test\n\ndata:  free_kick_accuracy by preferred_foot\nt = 40.1, df = 42148, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group left and group right is not equal to 0\n95 percent confidence interval:\n 4.999282 5.513107\nsample estimates:\n mean in group left mean in group right \n           53.52865            48.27246",
    "crumbs": [
      "Home",
      "Soccer",
      "Does Footedness Impact Free-Kick Accuracy?"
    ]
  },
  {
    "objectID": "soccer/Footedness/index.html#wrap-upconclusions",
    "href": "soccer/Footedness/index.html#wrap-upconclusions",
    "title": "Does Footedness Impact Free-Kick Accuracy?",
    "section": "5. Wrap-Up/Conclusions:",
    "text": "5. Wrap-Up/Conclusions:\n:\nThrough an exploration of the data and the usage of a two-sample t-test to analyze the data, we were able to reject the null hypothesis that there is no difference in free kick accuracy score between right-footed and left-footed players and show evidence for the alternative that there is a difference in average free-kick accuracy between these two groups. The p-value of 0 is less than our significance level of 0.01, and it shows that it is extremely unlikely that the observed difference between these two groups was due to chance alone. The confidence interval calculated shows that we can be 99% confident that the actual difference in free kick accuracy score between right-footed and left-footed players is between the values of 4.91388 and 5.40612, with left-footed kickers performing better.\n:\nTo recap, in this lesson, you learned how to conduct a statistical investigation by following the six-step statistical investigation method to answer the research question of whether there exists a difference in means between left-footed and right-footed players with regard to free-kick accuracy score. We went through how to manually perform a two-sample t-test for a difference in means between two groups as well as a built in R function to do the same thing. Conducting these sort of tests is one of the fundamental building blocks of statistics and can be applied to many different situations. Regarding other sports, many other sports, including baseball, tennis, golf, and basketball all relate to an athlete’s dominant side. The same process that was used in this module can be used to investigate these other sports. Another way to further your understanding of statistics would be to consider other variables in soccer as well.",
    "crumbs": [
      "Home",
      "Soccer",
      "Does Footedness Impact Free-Kick Accuracy?"
    ]
  },
  {
    "objectID": "soccer/Footedness/index.html#appendix",
    "href": "soccer/Footedness/index.html#appendix",
    "title": "Does Footedness Impact Free-Kick Accuracy?",
    "section": "6. Appendix:",
    "text": "6. Appendix:\n\nlibrary(tidyverse) \nlibrary('RSQLite') # SQLite package for R\nlibrary(DBI) # R Database Interface.\nlibrary(knitr)\nlibrary(janitor)\n\n#Connect to Database\ndatabaseConnection &lt;- dbConnect(drv=RSQLite::SQLite(), dbname=\"database.sqlite\")\n#List Tables\ntables &lt;- dbListTables(databaseConnection)\n#exclude sqlite_sequence (contains table information)\ntables &lt;- tables[tables != \"sqlite_sequence\"]\nlDataFrames &lt;- vector(\"list\", length=length(tables))\n#Create data frame for each table\nfor (i in seq(along=tables)) {\n  lDataFrames[[i]] &lt;- dbGetQuery(conn=databaseConnection, \n                                 statement=paste(\"SELECT * FROM '\", tables[[i]], \"'\", sep=\"\"))\n}\n#label all of the dataframes\nCountry &lt;- lDataFrames[[1]]\nLeague &lt;- lDataFrames[[2]]\nMatch &lt;- lDataFrames[[3]]\nPlayer &lt;- lDataFrames[[4]]\nPlayer_Attributes &lt;- lDataFrames[[5]]\nTeam &lt;- lDataFrames[[6]]\nTeam_Attributes &lt;- lDataFrames[[7]]\n\n#show the first 5 entries of each table\nCountry %&gt;%\n  head(5) %&gt;%\n  kable()\nLeague %&gt;%\n  head(5) %&gt;%\n  kable()\nMatch %&gt;%\n  head(5) %&gt;%\n  kable()\nPlayer %&gt;%\n  head(5) %&gt;%\n  kable()\nPlayer_Attributes %&gt;%\n  head(5) %&gt;%\n  kable()\nTeam %&gt;%\n  head(5) %&gt;%\n  kable()\nTeam_Attributes %&gt;%\n  head(5) %&gt;%\n  kable()\n\n#create csv file containing player attributes\nwrite.csv(Player_Attributes,file='./Player_Attributes.csv')",
    "crumbs": [
      "Home",
      "Soccer",
      "Does Footedness Impact Free-Kick Accuracy?"
    ]
  },
  {
    "objectID": "external_submissions/Robotics/index.html",
    "href": "external_submissions/Robotics/index.html",
    "title": "FIRST Robotics Competition - Winning Chances",
    "section": "",
    "text": "Introduction\nThe FIRST Robotics Competition (FRC) is a high school level robotics competition, in which “[u]nder strict rules and limited time and resources, teams of high school students are challenged to build industrial-size robots to play a difficult field game in alliance with other teams.” It combines “the excitement of sport with the rigors of science and technology”.\nOne of the key features of FRC is that robot/team competes not individually, but in alliance with other teams. So, it is important for teams to “scout” other teams as potential alliance partners. Various methodologies/models to evaluate each team’s potential contribution were developed. One of the popular models is called Expected Points Added (EPA) model.\nDetailed algorithm of the EPA model can be found at here (https://www.statbotics.io/blog/epa). Briefly, the EPA model builds upon the Elo rating system which is a “well-known method for ranking chess players, and has been adapted to many other domains.” It produces predicted probabilities of winning for the alliance based on the past performances of each team in the alliance, as well as teams in the opposition alliance. As such, there is a desire/need to assess how good the EPA model prediction is.\nBrier score originated with weather forecast research. It was designed to evaluate the predicted probabilities against the actual outcomes and is straight forward to calculate. While it is not widely used outside specific use cases, it is one of many approaches for the important step of evaluating models based on their predictions. Since the EPA model provides the predicted winning probabilities, Brier score is useful for evaluating its performance by comparing its predicted winning probabilities to the actual FRC outcomes.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nBy the end of this activity, you will be able to:\n\nCalculate Brier score.\nInterpret Brier score.\n\n\n\n\n\n\nData\nIn this lesson, we will use the EPA data and competition outcomes calculated and compiled by the website statbotics.io.\n\ndtAll &lt;- read.csv(\"matches.csv\")\ndtUse &lt;- subset(dtAll, status==\"Completed\" & offseason==\"f\", \n                select=c(year, event, playoff, comp_level, winner, epa_win_prob))\n\nBelow is a description of the variables:\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nyear\nthe year/season of the FRC event\n\n\nevent\nunique identifier for each FRC event\n\n\nplayoff\n“t” for playoff match; “f” for qualifying match\n\n\ncomp_level\n“qm” for qualifying match; “sf” for semifinals match; “f” for finals match\n\n\nwinner\nwinning alliance (“red” or “blue”) of the match\n\n\nepa_win_prob\npredicted winning probability for the Red Alliance by EPA model\n\n\n\n\nThe data covers the competition seasons from 2002 to 2023, except for 2021 due to the COVID pandemic.\n\nunique(dtUse$year)\n\n [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n[16] 2017 2018 2019 2020 2022 2023\n\n\nFor our example, we will use a particular event, the Hopper Division competition at the 2023 FRC World Championship in Houston, to illustrate the calculation of Brier score.\n\ndt2023hop &lt;- subset(dtUse, event==\"2023hop\")\n\nBelow is what the raw data looks like. Each row is a match between a Red alliance and a Blue alliance. Each alliance consists of three robots/teams.\n\n\n\n\n\n\n\n\nBrier Score\nFor match \\(i\\), let \\(f_i\\) denote the probability forecast. In our case, it is the predicted winning probability for the Red Alliance by EPA model, i.e., the variable epa_win_prob. Let \\(o_i\\) denote the match outcome: \\(o_i=1\\) when the Red alliance won and \\(o_i=0\\) when the Blue alliance won. The Brier score for match \\(i\\) is calculated as \\((f_i - o_i)^2\\). For example, suppose it is predicted that the Red alliance will win with 80% probability, i.e., \\(f_i=0.8\\), if the actual outcome is that the Red alliance won, the Brier score is \\((0.8-1)^2=0.04\\). If the actual outcome is that the Blue alliance won, the Brier score is \\((0.8-0)^2=0.64\\).\nBrier score is a quantity bounded by \\(0\\) and \\(1\\). Brier score of \\(0\\) means correctly predicting the outcome with 100% certainty. 50:50 random guess would give a Brier score of \\(0.25\\). The overall Brier score for all the matches during a competition event or season is simply the average of individual match scores: \\[\\frac{1}{N} \\sum_{i=1}^N (f_i - o_i)^2\\] The following table shows the calculation for each match.\n\n\n\n\n\n\nThe overall Brier score for the 2023 Hopper Division event is 0.1366352, which is slightly worse than the half way between perfect \\(0\\) and random guess \\(0.25\\).\n\n\nYour Turn\nNow, it’s your turn. Please use the data from the Turing Division competition at the 2022 FRC World Championship to calculate the average Brier score for the event. You should find the data in the file dt2022tur.csv.\n\ndt2022tur &lt;- subset(dtUse, event==\"2022tur\")\nwrite.csv(dt2022tur, \"dt2022tur.csv\")\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe average Brier score for the 2022 Turing Division event is 0.1160236, which is better than the Brier score for the 2023 Hopper Division event.\n\n\n\n\n\nOver the Years\nSince we have the data for more than 20 years, we could answer an interesting question: did the predictive ability of the EPA model change over the years?\nWe build two simple functions to do the calculations.\n\n\nTwo Functions\n\nThe first function calculates the Brier score for a given data set. Occasionally, a game can end in a draw. We assign the value of \\(0.5\\) to \\(o_i\\) for a draw.\n\ncalcBS &lt;- function(dt){\n  n &lt;- nrow(dt)\n  outcome &lt;- rep(NA, n)\n  outcome[dt$winner==\"red\"] &lt;- 1\n  outcome[dt$winner==\"draw\"] &lt;- 0.5\n  outcome[dt$winner==\"blue\"] &lt;- 0\n  diff &lt;- dt$epa_win_prob - outcome\n  Brier &lt;- mean(diff^2)\n  c(n=n, Brier=Brier)\n}\n\nThe second function separates the data by year and does the calculation for each year.\n\nbyYear &lt;- function(dt=dtUse) {\n  yrs &lt;- unique(dt$year)\n  m &lt;- length(yrs)\n  size &lt;- Brier &lt;- rep(NA, m)\n  for (i in 1:m) {\n    dat &lt;- subset(dt, year==yrs[i])\n    res &lt;- calcBS(dt=dat)\n    size[i] &lt;- res[1]\n    Brier[i] &lt;- res[2]\n  }\n  data.frame(year=yrs, n=size, Brier=Brier)\n}\nFRC &lt;- byYear()\n\n\nBelow are the Brier scores from 2002 to 2023.\n\nknitr::kable(FRC)\n\n\n\n\nyear\nn\nBrier\n\n\n\n\n2002\n2197\n0.2351889\n\n\n2003\n3173\n0.2225493\n\n\n2004\n3198\n0.2069319\n\n\n2005\n2059\n0.2043631\n\n\n2006\n3283\n0.1997467\n\n\n2007\n3563\n0.2099309\n\n\n2008\n4036\n0.1892942\n\n\n2009\n4567\n0.1961946\n\n\n2010\n5564\n0.1691369\n\n\n2011\n6224\n0.1621286\n\n\n2012\n7707\n0.1841302\n\n\n2013\n8242\n0.1704309\n\n\n2014\n10663\n0.1906669\n\n\n2015\n11810\n0.1841460\n\n\n2016\n13286\n0.1794790\n\n\n2017\n15429\n0.2043697\n\n\n2018\n16930\n0.1750251\n\n\n2019\n18022\n0.1758972\n\n\n2020\n4634\n0.1817734\n\n\n2022\n14645\n0.1480655\n\n\n2023\n16319\n0.1604984\n\n\n\n\n\nIt is interesting to note that the predictive ability of the EPA model has improved for the past 20 years. Since the model has not changed, I believe the improvement comes from established teams becoming more consistent and predictable. Meanwhile, the pool of newer, less experienced teams has stayed healthy.\n\nplot(FRC$year, FRC$Brier)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn conclusion, Brier score is a simple statistic that assesses the probability prediction against actual outcome. A smaller Brier score corresponds to better prediction.\nThe EPA model has been getting better at predicting FRC match outcome.\n\n\n\n\n\nAuthor\nCreated by Jake Tan (Wissahickon High School). Jake is a subsystem leader at FRC Team 341, Miss Daisy. Team 341 competed at FRC World Championship in the Turing Division in 2022 and Hopper Division in 2023.",
    "crumbs": [
      "Home",
      "External Submissions",
      "FIRST Robotics Competition - Winning Chances"
    ]
  },
  {
    "objectID": "baseball/predicting-all-stars/index.html",
    "href": "baseball/predicting-all-stars/index.html",
    "title": "Predicting Allstars",
    "section": "",
    "text": "Understand True Positive (Sensitivity) and True Negative (Specificity) Rates (TPR and TNR, respectively)\nUnderstand Positive and Negative Predictive Values (PPV and NPV, respectively)\nUnderstand F1 Score\nTrain a predictive model and test it on new data\nUtilize TPR, TNR, PPV, NPV, and F1 Score to assess model adequacy",
    "crumbs": [
      "Home",
      "Baseball",
      "Predicting Allstars"
    ]
  },
  {
    "objectID": "baseball/predicting-all-stars/index.html#learning-objectives",
    "href": "baseball/predicting-all-stars/index.html#learning-objectives",
    "title": "Predicting Allstars",
    "section": "",
    "text": "Understand True Positive (Sensitivity) and True Negative (Specificity) Rates (TPR and TNR, respectively)\nUnderstand Positive and Negative Predictive Values (PPV and NPV, respectively)\nUnderstand F1 Score\nTrain a predictive model and test it on new data\nUtilize TPR, TNR, PPV, NPV, and F1 Score to assess model adequacy",
    "crumbs": [
      "Home",
      "Baseball",
      "Predicting Allstars"
    ]
  },
  {
    "objectID": "baseball/predicting-all-stars/index.html#introduction",
    "href": "baseball/predicting-all-stars/index.html#introduction",
    "title": "Predicting Allstars",
    "section": "Introduction",
    "text": "Introduction\nThere are many players in the MLB that are considered “perennial All Stars.” However, every year there are a handful of players that make the All Star game in an unexpected fashion. All Star game selection does not have a set standard for a player’s performance, but rather seems to rely on their popularity among fans. This module aims to see if there is a way to predict which players are poised to make the All Star game based on the statistics of their previous season.\nIn order to accomplish this, a model must be trained by a data set. This trained model is then applied to “unseen” data that has been split from the same data set as the training data. After the new data has been run against the model, the adequacy of the model can be judged through measures including True Positive and True Negative Rates (Sensitivity and Specificity, respectively), Positive and Negative Predictive Values, and the F1 Score. A further explanation of each measure is provided in the Methods/Instructional Content section.\nIn order to calculate these measures, a logistic regression model can be used. The different statistics that will be used to classify whether a player would be anticipated to make the All Star game or not are batting average (AVG), home runs (HR), on-base percentage (OBP), and slugging percentage (SLG). A further explanation of AVG, OBP, and SLG will be provided in the Data section.",
    "crumbs": [
      "Home",
      "Baseball",
      "Predicting Allstars"
    ]
  },
  {
    "objectID": "baseball/predicting-all-stars/index.html#data",
    "href": "baseball/predicting-all-stars/index.html#data",
    "title": "Predicting Allstars",
    "section": "Data",
    "text": "Data\nThe Lahman database is a set of data frames including pitching, hitting, fielding, and other information. All statistics from 1871 to 2022 are present in the Lahman database. One major benefit of using the Lahman database is the ability to easily join the tables on year or player names. In order to use the Lahman database, the first step is to install its package. Then, both the Batting and AllstarFull tables must be obtained.\n\n#install.packages(\"Lahman\")\nlibrary(Lahman)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(caret)\n\n#Save the Batting and AllstarFull tables\n#Under usable names\nall_hitters &lt;- Batting\nasg_apps &lt;- AllstarFull\n\nThe next step is to mutate the provided columns in the Batting table to produce columns for AVG, OBP, and SLG. The formulas for each are provided below:\nBatting Average = \\(AVG = \\frac{H}{AB}\\).\nOn Base Percentage = \\(OBP = \\frac{H + BB + HBP}{AB + BB + HBP + SF}\\).\nSlugging Percentage = \\(SLG = \\frac{TB}{AB} = \\frac{1B + 2*2B + 3*3B + 4*HR}{AB}\\)\n\n#Mutate the above stats into the hitters table\nall_hitters &lt;- all_hitters %&gt;% \n  mutate(AVG = H/AB,\n          OBP = (H + BB + HBP)/(AB + BB + HBP + SF),\n         SLG = (H - X2B - X3B - HR +\n             2 * X2B + 3 * X3B + 4 * HR)/AB)\n\nThe next step is to subtract 1 from the yearID column in the All Star Game table. Also, add a new column that has a value of 1 for all members in the All Star table to show that a given player was an all star. This allows us to see if a player’s statistics in one season correspond to their selection to the All Star game in the next. After this, join the two tables by year and player.\n\n#Subtract one year from the all star table\n#To enable it to be joined correctly and then\n#add a column that marks every player in the table\n#with a \"1\" for being an all star in that season\nasg_apps &lt;- asg_apps %&gt;% \n  mutate(yearID = yearID - 1) %&gt;% \n  mutate(allstar = 1)\n\n#Join the two tables by year and player\nall_star_hitters &lt;- all_hitters %&gt;% \n  left_join(asg_apps, by = c(\"yearID\", \"playerID\"), relationship = \"many-to-many\")\n\nFilter out all seasons prior to 2000, as well as the 2022 season (because this season does not have a corresponding column in the All Star table). Select the variables needed to make the regression model (playerID, yearID, AVG, HR, OBP, SLG, and gameNum) Finally, turn all NAs in the data to zeroes to ensure the regression model can run.\n\n#Filter the data set so it contains the \n#2000-2021 seasons\n#Filter out all players with less than 200 ABs\n#select the stats to be used for model\nall_star_hitters &lt;- all_star_hitters %&gt;% \n  filter(yearID &gt;= 2000) %&gt;% \n  filter(yearID &lt; 2022) %&gt;% \n  filter(AB &gt; 200) %&gt;% \n  select(playerID, yearID, AVG, HR, OBP, SLG, allstar)\n\nall_star_hitters[is.na(all_star_hitters)] &lt;- 0\n#Used FavTutor source to implement above line.\n\nall_star_hitters %&gt;% \n  summary() %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nAVG\nHR\nOBP\nSLG\nallstar\n\n\n\n\n\nLength:6885\nMin. :2000\nMin. :0.1429\nMin. : 0.00\nMin. :0.1745\nMin. :0.1867\nMin. :0.0000\n\n\n\nClass :character\n1st Qu.:2005\n1st Qu.:0.2449\n1st Qu.: 7.00\n1st Qu.:0.3087\n1st Qu.:0.3768\n1st Qu.:0.0000\n\n\n\nMode :character\nMedian :2010\nMedian :0.2654\nMedian :12.00\nMedian :0.3316\nMedian :0.4228\nMedian :0.0000\n\n\n\nNA\nMean :2010\nMean :0.2650\nMean :14.09\nMean :0.3332\nMean :0.4285\nMean :0.1217\n\n\n\nNA\n3rd Qu.:2015\n3rd Qu.:0.2860\n3rd Qu.:20.00\n3rd Qu.:0.3564\n3rd Qu.:0.4742\n3rd Qu.:0.0000\n\n\n\nNA\nMax. :2021\nMax. :0.3724\nMax. :73.00\nMax. :0.6094\nMax. :0.8634\nMax. :1.0000\n\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nAVG\nHR\nOBP\nSLG\nallstar\n\n\n\n\nabbotje01\n2000\n0.274\n3\n0.343\n0.395\n0\n\n\nabreubo01\n2000\n0.316\n25\n0.416\n0.554\n0\n\n\nagbaybe01\n2000\n0.289\n15\n0.391\n0.477\n0\n\n\nalfoned01\n2000\n0.324\n25\n0.425\n0.542\n0\n\n\nalicelu01\n2000\n0.294\n6\n0.365\n0.404\n0\n\n\nalomaro01\n2000\n0.310\n19\n0.378\n0.475\n1\n\n\nalomasa02\n2000\n0.289\n7\n0.324\n0.404\n0\n\n\naloumo01\n2000\n0.355\n30\n0.416\n0.623\n1\n\n\nanderbr01\n2000\n0.257\n19\n0.375\n0.421\n0\n\n\nanderga01\n2000\n0.286\n35\n0.307\n0.519\n0",
    "crumbs": [
      "Home",
      "Baseball",
      "Predicting Allstars"
    ]
  },
  {
    "objectID": "baseball/predicting-all-stars/index.html#methodsinstructional-content",
    "href": "baseball/predicting-all-stars/index.html#methodsinstructional-content",
    "title": "Predicting Allstars",
    "section": "Methods/Instructional Content",
    "text": "Methods/Instructional Content\nThere are many measures that can be used to assess the performance of a model. Some of these measures include Sensitivity, Specificity, Positive Predictive Value (PPV), Negative Predictive Value (NPV), and F1 Score.\nThe Monaghan et. al journal article provides a great deal of insight into what the sensitivity, specificity, PPV, and NPV represent.\nA “True Positive” represents a data point that is predicted to be True by the model and is actually True. A “True Negative” represents a data point that is predicted to be False by the model and is actually False. A “False Positive” is a data point that is predicted to be True by the model, but is in fact False. A “False Negative” is a data point that is predicted to be False by the model, but is actually True.\nThe Crandon article summarizes what each measure shown above represents. Sensitivity indicates how likely a model is to classify something as true if the data point is actually true. Specificity shows how likely a model is to classify something as false if the data point is actually false. The PPV represents how likely a data point is to be true if the model classifies it as true. The NPV indicates how likely a data point is to be false if the model classifies it as false. The formulas for each are as follows:\n\\(Sensitivity = \\frac{True Positives}{True Positives + False Negatives}\\).\n\\(Specificity = \\frac{True Negatives}{True Negatives + False Positives}\\).\n\\(PPV = \\frac{True Positives}{True Positives + False Positives}\\).\n\\(NPV = \\frac{True Negatives}{True Negatives + False Negatives}\\).\nFinally, the Kundu article summarizes how the calculate the F1 score of a model and what this score represents. The F1 score is used to assess the accuracy of a model, relating to the number of correct predictions a model made. The F1 score is a combination of the Precision and Recall of the model. Precision looks at how many True predictions made by the model were True in reality. The Recall of a model is how often values that are True in reality were classified as True by the model. The formula for Precision is the same as that of PPV, and the formula of Recall is the same as Sensitivity. A higher F1 Score for a model indicates high values for both Precision and Recall. The F1 Score indicates how well a model is at classifying values based on what they are in reality and that the predictions are in fact correct.\nUsing the formulas for Precision and Recall, the formula for F1 Score is\n\\(F1 Score = \\frac{2}{(Precision)^{-1}+(Recall)^{-1}}\\)\nOr simplified as\n\\(F1 Score = \\frac{True Positives}{True Positives + .5(False Positives + False Negatives)}\\)\nAll three of these sources provided background and explanations of the aforementioned scores and how each is used to assess a model.",
    "crumbs": [
      "Home",
      "Baseball",
      "Predicting Allstars"
    ]
  },
  {
    "objectID": "baseball/predicting-all-stars/index.html#exercisesactivities",
    "href": "baseball/predicting-all-stars/index.html#exercisesactivities",
    "title": "Predicting Allstars",
    "section": "Exercises/Activities",
    "text": "Exercises/Activities\nFirst, split the data set into a training and testing set.\n\n\n\n\n\n\nAnswer 1\n\n\n\n\n\n\nset.seed(679)\nsplit&lt;- createDataPartition(all_star_hitters$allstar, p = .75, list = FALSE)\ntrain &lt;- all_star_hitters[split,]\ntest &lt;- all_star_hitters[-split,]\n\n\n\n\nNext, create a logistic regression model that classifies a hitter as an All Star or not using AVG, HR, OBP, and SLG.\n\n\n\n\n\n\nAnswer 2\n\n\n\n\n\n\nall_star_mod &lt;- glm(allstar ~ AVG + HR + OBP + SLG, data = train,\n                    family = \"binomial\")\n\n\n\n\nNow that you have trained a logistic regression model that quantifies a player’s All Star status, use the test data set created above to predict whether a hitter would be an All Star or not using the model.\n\n\n\n\n\n\nAnswer 3\n\n\n\n\n\n\npred &lt;- predict(all_star_mod, newdata = test, type = \"response\")\nsummary(pred)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.004244 0.041694 0.075333 0.115808 0.141457 0.845290 \n\n\n\n\n\nCreate a matrix that contains the number of each value (True Positive etc.). Use a threshold of 0.2 to classify whether a player is an All Star or not.\n\n\n\n\n\n\nAnswer 4\n\n\n\n\n\n\nthreshold &lt;- 0.2\nallstar_matrix &lt;- table(test$allstar, pred &gt;= threshold)\nallstar_matrix %&gt;% \n  kable(col.names = )\n\n\n\n\n\nFALSE\nTRUE\n\n\n\n\n0\n1344\n163\n\n\n1\n124\n90\n\n\n\n\n\n\n\n\nFinally, calculate each of the metrics from above (Sensitivity etc.). Reference the example Confusion Matrix to determine which values are needed to compute each value.\n\n\n\n\n\n\nAnswer 5\n\n\n\n\n\n\nsensitivity &lt;- allstar_matrix[2,2]/(allstar_matrix[2,2] + allstar_matrix[2,1])\n\nspecificity &lt;- allstar_matrix[1,1]/(allstar_matrix[1,1] + allstar_matrix[1,2])\n\nppv &lt;- allstar_matrix[2,2]/(allstar_matrix[2,2] + allstar_matrix[1,2])\n\nnpv &lt;- allstar_matrix[1,1]/(allstar_matrix[1,1] + allstar_matrix[2,1])\n\nfscore &lt;- allstar_matrix[2,2]/(allstar_matrix[2,2] + .5*(allstar_matrix[1,2] + allstar_matrix[2,1]))\n\n\n\n\nShow a table of each value, labeled with its respective measure.\n\n\n\n\n\n\nAnswer 6\n\n\n\n\n\n\nvalues &lt;- c(sensitivity, specificity, ppv, npv, fscore)\nmeasures &lt;- c(\"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\", \"F1 Score\")\nstats &lt;- data.frame(`measures`, `values`)\nstats %&gt;% \n  kable(digits = 4)\n\n\n\n\nmeasures\nvalues\n\n\n\n\nSensitivity\n0.4206\n\n\nSpecificity\n0.8918\n\n\nPPV\n0.3557\n\n\nNPV\n0.9155\n\n\nF1 Score\n0.3854\n\n\n\n\n\n\n\n\nAs shown above, the model is exceptionally good at predicting false values. This is shown by high Specificity and NPV. As a reminder, Sensitivity indicates how likely a model is to classify something as true if the data point is actually true while NPV represents how likely a data point is to be false if the model classifies it as false.\nHowever, the model is extremely lacking in its ability to predict true values, as shown by low Sensitivity and PPV values. To reiterate, Sensitivity indicates how likely a model is to classify something as true if the data point is actually true and PPV represents how likely a data point is to be true if the model classifies it as true.\nAdditionally, the F1 Score of this model is very low, indicating that this model is not very accurate. Again, F1 score represents the number of correct predictions a model made. This indicates that this model may not be very useful at classifying players as All Stars or not depending on their prior season’s statistics.",
    "crumbs": [
      "Home",
      "Baseball",
      "Predicting Allstars"
    ]
  },
  {
    "objectID": "baseball/predicting-all-stars/index.html#wrap-upconclusions",
    "href": "baseball/predicting-all-stars/index.html#wrap-upconclusions",
    "title": "Predicting Allstars",
    "section": "Wrap-Up/Conclusions",
    "text": "Wrap-Up/Conclusions\nThis lesson introduced how to create a logistic regression model, with this model then being used to classify unseen data. Finally, the model’s adequacy was assessed on its ability to classify the unseen data through measures of Sensitivity, Specificity, Positive Predictive Value, and Negative Predictive Value. This was all applied to how well a logistic regression model could classify a player as an All Star or not depending on his statistics the previous season.\nAs shown, logistic regression is not very successful at classifying a player as an All Star by using the previous season’s statistics. This is likely because of season-to-season fluctuations in “successful” player performance. Statistics that could be considered good in one year may be considered average in another. One way to make this model stronger would be to standardize all statistics in order to have a common comparison from season to season.\nThe biggest limitation of this exercise is that the data set contains a much larger number of players that are not All Stars than players that are All Stars. This is because there are very few players that are selected to the All Star Game every year. One potential mitigation of this issue would be to increase the minimum number of at bats for a player to be considered in the analysis. This would filter out even more players that had no chance of making the All Star game, likely leading to higher values of the performance measures described above.\nAnother sports application for this would be classifying a player as a Hall of Famer or not depending on their career statistics and then assessing how adequate the logistic regression model is by testing it on unseen data. This might work better because although no performance benchmarks have been set to be selected for the Hall of Fame, the performance of inductees is much more consistent than players selected to be All Stars.\nAn additional skill that builds on this idea is to calculate the Out-of-Sample R Squared, Root Mean Square Error, and Mean Absolute Error to further assess how strong a model is at classifying unseen data. These values could then be compared to other models using the same data, but with altered filters on the minimum number of ABs to see what benchmark for ABs is the most appropriate.",
    "crumbs": [
      "Home",
      "Baseball",
      "Predicting Allstars"
    ]
  },
  {
    "objectID": "BlackJack/BlackJack_Log_Reg/index.html",
    "href": "BlackJack/BlackJack_Log_Reg/index.html",
    "title": "BlackJack Logistic Regression",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThis module is available on the ISLE platform BlackJack Module",
    "crumbs": [
      "Home",
      "BlackJack",
      "BlackJack Logistic Regression"
    ]
  },
  {
    "objectID": "BlackJack/BlackJack_Log_Reg/index.html#module",
    "href": "BlackJack/BlackJack_Log_Reg/index.html#module",
    "title": "BlackJack Logistic Regression",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThis module is available on the ISLE platform BlackJack Module",
    "crumbs": [
      "Home",
      "BlackJack",
      "BlackJack Logistic Regression"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "West Point SCORE Module Pre-print Repository",
    "section": "",
    "text": "This page contains education materials for the SCORE Network that were created by faculty and students from the Department of Mathematical Sciences at West Point.\nThe SCORE Network Module Repository enables you to search for modules by either sport (along the left), or you can browse by statistics and data science topic.\n\n\n\n\n\n\nDisclaimer\n\n\n\n\n\nPlease note that these material have not yet completed the required pedagogical and industry peer reviews to become a published module on the SCORE Network.\nHowever, all modules listed under their respective sports have been reviewed by a West Point statistics instructor and are able for others to use.\n\n\n\nThe development of the SCORE with Data network is funded by the National Science Foundation (award 2142705).\n\n\n\n\n\n\nContributing and/or Joining the SCORE Network\n\n\n\n\n\n\nIf you are interested in contributing to and/or joining the SCORE Network, please check out https://scorenetwork.org/index.html.\nThis page was based off of the work done by https://github.com/iramler/slu_score_preprints.\nThe basic structure of the this repository is to have a directory for each sport, then within the sport, each module will have a folder containing its files. We have found that this allows us to easily submit a module in this repository to the SCORE Network and requires only minimal work for preparing it for the SCORE module repository after it is accepted."
  },
  {
    "objectID": "_team.html",
    "href": "_team.html",
    "title": "The Team",
    "section": "",
    "text": "Nick Clark\n\n\n\n\n\nNick Clark is pretty cool\n\n\n\n\n\n\n\n\n\nAndy Lee\n\n\n\n\n\nAndy Lee is another Guy\n\n\n\n\n\n\n\n\n\nRachel Gidaro\n\n\n\n\n\nRachel will probably do most of the work\n\n\n\n\n\n\nWe’ve had numerous Cadets assist in developing both SCORE modules and preparing submissions for the data repository. They are listed by year below.\n\n\n\n\n\n\n2024\n\n\n\n\n\nTBD\n\n\n\n\n\n\n\n\n\n2023\n\n\n\n\n\nTBD",
    "crumbs": [
      "Home",
      "The Team"
    ]
  },
  {
    "objectID": "_team.html#faculty",
    "href": "_team.html#faculty",
    "title": "The Team",
    "section": "",
    "text": "Nick Clark\n\n\n\n\n\nNick Clark is pretty cool\n\n\n\n\n\n\n\n\n\nAndy Lee\n\n\n\n\n\nAndy Lee is another Guy\n\n\n\n\n\n\n\n\n\nRachel Gidaro\n\n\n\n\n\nRachel will probably do most of the work",
    "crumbs": [
      "Home",
      "The Team"
    ]
  },
  {
    "objectID": "_team.html#students",
    "href": "_team.html#students",
    "title": "The Team",
    "section": "",
    "text": "We’ve had numerous Cadets assist in developing both SCORE modules and preparing submissions for the data repository. They are listed by year below.\n\n\n\n\n\n\n2024\n\n\n\n\n\nTBD\n\n\n\n\n\n\n\n\n\n2023\n\n\n\n\n\nTBD",
    "crumbs": [
      "Home",
      "The Team"
    ]
  },
  {
    "objectID": "Lacrosse/expected_goals/index.html",
    "href": "Lacrosse/expected_goals/index.html",
    "title": "Expected Goals in Lacrosse",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThis module is available on the ISLE platform Expected Goals in Lacrosse Module",
    "crumbs": [
      "Home",
      "Lacrosse",
      "Expected Goals in Lacrosse"
    ]
  },
  {
    "objectID": "Lacrosse/expected_goals/index.html#module",
    "href": "Lacrosse/expected_goals/index.html#module",
    "title": "Expected Goals in Lacrosse",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThis module is available on the ISLE platform Expected Goals in Lacrosse Module",
    "crumbs": [
      "Home",
      "Lacrosse",
      "Expected Goals in Lacrosse"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html",
    "href": "basketball/nba-ordinal-regression/index.html",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "",
    "text": "Solutions to this module are available here\nHere is the data associated with this module",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#sports-context",
    "href": "basketball/nba-ordinal-regression/index.html#sports-context",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Sports Context",
    "text": "Sports Context\nWe will apply Ordinal Logistic Regression into predicting the best salary to pay NBA players based on their previous skill. Every year, teams in the NBA assign contracts to players based on their previous performance. It’s important for NBA teams, who hire data scientists, to strike a balance between properly compensating players for their skill and also reserving money and staying under budget. However, learners should have an upfront understanding of the limitations of our model: it does not account for rookie players, and only applies to players who are no longer on rookie contracts. We wanted to keep this as simple as possible for those trying to understand Ordinal Logistic Regression.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#sports-application",
    "href": "basketball/nba-ordinal-regression/index.html#sports-application",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Sports Application",
    "text": "Sports Application\nEach year, the NBA as a whole makes upwards of $10.58 billion in revenue. WOW! Have you ever wondered where all of this money goes or how teams determine how much money each player gets paid? Believe it or not, the allocation of this funding based on the relevant skills and talents of the players is important for nurturing player development while ensuring the NBA’s competitive balance. Over time, in order to have a good team and thus make more money, owners and general managers must compensate their players adequately so that they do not join other teams. However, teams must have enough money for self-sustainment and funding events. As such, the unique balance arises whereby teams must pay players just enough to satisfy them while ensuring that the team makes a profit off of their games and events.\nAs a preliminary measure of finding the best salary to pay players based on previous skill, we aim to determine which factors can predict the bin of an NBA salary. As such, this bin will have three levels: low, medium, and high. We feel that this model is a precursor to someone understanding how NBA teams ensure efficiency of monetary allocation and thus team financial success.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#statistical-technique",
    "href": "basketball/nba-ordinal-regression/index.html#statistical-technique",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Statistical Technique",
    "text": "Statistical Technique\nOrdinal Logistic Regression is a form of Logistic Regression in which one can model a relationship between an ordinal response variable (NBA salary bin: low, medium, high) and one or more explanatory variables (player statistics, performance, and more).\n\nWhat about Logistic Regression?\nIt’s important to also understand before we get started that logistic regression essentially estimates the probability of an event occurring, such as whether someone does something or not. In contrast to linear regression, logistic regression has a binary (or more than two binned) response variable.\n\n\nWhy Ordinal Regression Here?\nThe difference between Ordinal and Logistic Regression is that in ordinal, the response variable has a specific order. For example, in this case, the “order” is “low, medium, and high” salary. Another example of this might be survey responses: for example, “bad, alright, awesome” which need to be in the correct order and are also not numerically spaced. One final example would be predicting the stages of a disease. For example, in trying to predict the stages of cancer based on previous patient data, order of predictions would matter because such order informs the level of metastasis of the cancer. As you can probably tell, Ordinal Logistic regression is more complex than normal logistic regression due to the nature of having multiple categories and thus thresholds.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#instructions",
    "href": "basketball/nba-ordinal-regression/index.html#instructions",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Instructions",
    "text": "Instructions\nIn this module, you will use R and its associated packages to perform Ordinal Logistic Regression. Follow along below. Some code will be given to you and some you will have to determine for yourself. The steps should logically flow. Make sure you complete all steps in order so that you are not confused from skipping around.\nFirst, we need to load the required packages. You may need to install packages. If that is the case, uncomment the install packages line of code below and insert the necessary packages that you need to download.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(corrplot)\n\ncorrplot 0.94 loaded\n\nlibrary(foreign)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n#install.packages(c('package1','package2','package3'...))",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#what-data-was-used-and-where-did-it-come-from",
    "href": "basketball/nba-ordinal-regression/index.html#what-data-was-used-and-where-did-it-come-from",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "What Data was Used and Where did it Come From?",
    "text": "What Data was Used and Where did it Come From?\nWe will now read the NBA data. The data is linked at the following cite which also has an explanation of what the variables in the data mean and various ways to analyze it. See below:\nhttps://towardsdatascience.com/predicting-nba-salaries-with-machine-learning-ed68b6f75566\nThe data on NBA players contains information on various measurables in professional basketball from the 2021-2022 NBA regular season. It contains 812 player-team stints for the season, meaning that there are 812 individual observations. Were a player to have played for multiple teams in the given season, they are recorded as unique observations. This is an advantage for ordinal logistic regression because we are looking to see the factors that determine the bin of a player’s salary. It is worth noting that this dataset is already “cleaned” in the sense that there are no missing, or NA, values. However, for your awareness, it is worth noting that NBA data is collected by Second Spectrum softward which tracks the movements of each player on the court and the basketball 25 times per second! Let’s go ahead and load in the data.\n\nbball &lt;- read_csv(\"NBA_Data.csv\")\n\nRows: 1264 Columns: 83\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Player, Tm, Season S-1\ndbl (80): Age, G, GS, MP_per_game, FG_per_game, FGA_per_game, FG%_per_game, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nkable(head(bball[, 1:7]))\n\n\n\n\nPlayer\nAge\nTm\nG\nGS\nMP_per_game\nFG_per_game\n\n\n\n\nPrecious Achiuwa\n23\nTOR\n55\n12\n20.7\n3.6\n\n\nSteven Adams\n29\nMEM\n42\n42\n27.0\n3.7\n\n\nBam Adebayo\n25\nMIA\n75\n75\n34.6\n8.0\n\n\nOchai Agbaji\n22\nUTA\n59\n22\n20.5\n2.8\n\n\nSanti Aldama\n22\nMEM\n77\n20\n21.8\n3.2\n\n\nNickeil Alexander-Walker\n24\nMIN\n59\n3\n15.0\n2.2",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#cleaning-and-preprocessing",
    "href": "basketball/nba-ordinal-regression/index.html#cleaning-and-preprocessing",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Cleaning and Preprocessing",
    "text": "Cleaning and Preprocessing\nIn the next few sections, you’ll spend time learning to clean and visualize the data. Data cleaning is often the most important part of any data science project, so pay close attention to what’s going on, and ensure that you answer the associated questions.\n\nVariable Selection\nHere, we are selecting the variables that we will use in this analysis. This includes Player, Team, Games Played, Games Started, Center Position, Power Forward Position, Point Guard Position, Small Forward Position, Shooting Guard Position, Field Goals Made per Game, 3-Point Field Goals Made per Game, Steals per Game, Blocks per Game, Points per Game, Total Points, Salary, and Age. Let’s also save it as a new data frame in order to preserve the original. It’s important when doing a study to document and even save “old” Dataframes so that if you need to retrace your steps of analysis you can easily do so by referencing old dataframes.\n\n# Get the data we actually need\nbball_clean &lt;- bball %&gt;% \n  dplyr::select(Player, Age, Tm, G, GS, C, PF, PG, SF, SG,\n         FG_per_game, `3P_per_game`, \n         STL_per_game, BLK_per_game, \n         PTS_per_game, PTS_totals,\n         `Salary S`)\n\n\n\nVariable Meaning Introduction\nIf you’re like the authors, you’re not necessarily a Basketball person and are also a bit confused on what each of these variables mean. Below is a definition of each. We want you to focus on the statistics of this instead of trying to understand what everything means! See below:\nPlayer: The name of the basketball player.\nTeam: The team the player is currently playing for.\nGames Played: The number of games the player has participated in.\nGames Started: The number of games the player has started. Center Position: A binary variable indicating whether the player primarily plays the center position (1 if yes, 0 if no).\nPower Forward Position: A binary variable indicating whether the player primarily plays the power forward position (1 if yes, 0 if no).\nPoint Guard Position: A binary variable indicating whether the player primarily plays the point guard position (1 if yes, 0 if no).\nSmall Forward Position: A binary variable indicating whether the player primarily plays the small forward position (1 if yes, 0 if no).\nShooting Guard Position: A binary variable indicating whether the player primarily plays the shooting guard position (1 if yes, 0 if no).\nField Goals Made per Game: The average number of field goals made by the player per game.\n3-Point Field Goals Made per Game: The average number of three-point field goals made by the player per game.\nSteals per Game: The average number of steals made by the player per game.\nBlocks per Game: The average number of shots blocked by the player per game.\nPoints per Game: The average number of points scored by the player per game.\nTotal Points: The total number of points scored by the player.\nSalary: The salary of the player in dollars.\nAge: The age of the player in years.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#variable-drawbacks",
    "href": "basketball/nba-ordinal-regression/index.html#variable-drawbacks",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Variable Drawbacks",
    "text": "Variable Drawbacks\nOne potential drawback of our selected variables is the lack of categorical features. Position is the only one that will be considered in the model. However, we are confident that the data is clean enough initially and well-organized. From the initial exploration, notice that the distributions for some variables, salary included, had been scaled down already. Another potential drawback is that we will be unable to create a model that considers years in the league or whether the player was on a rookie contract. This information simply isn’t in the data. Still, we can get a solid picture of the predictions due to the binning and we find that our model is relatively accurate in predicting player salary even despite these limitations.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#exercise-1",
    "href": "basketball/nba-ordinal-regression/index.html#exercise-1",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "EXERCISE 1",
    "text": "EXERCISE 1\nIn this, we recommend binning them based on percentiles. Because we want them in thirds, set the first bin at the 33rd percentile and the second and the 67th percentile. What are some other ways you might bin a numerical variable like salary?\n\n# Your Code Here\n\n#Create the quantiles and store them as perc_33 and perc_67\nperc_33 &lt;- quantile(bball_clean$`Salary S`, probs = 0.33)\nperc_67 &lt;- quantile(bball_clean$`Salary S`, probs = 0.67)\n\n#categrize into three categories based on where the values lie\n#for salary in the dataset: \"Low\", \"Medium\", or \"High\"\nbball_clean$sal_bin &lt;- ifelse(bball_clean$`Salary S` &lt;= perc_33, \"Low\",\n                              ifelse(bball_clean$`Salary S` &lt;= perc_67, \"Medium\", \"High\"))\n\nkable(head(bball_clean[, 1:7]))\n\n\n\n\nPlayer\nAge\nTm\nG\nGS\nC\nPF\n\n\n\n\nPrecious Achiuwa\n23\nTOR\n55\n12\n1\n0\n\n\nSteven Adams\n29\nMEM\n42\n42\n1\n0\n\n\nBam Adebayo\n25\nMIA\n75\n75\n1\n0\n\n\nOchai Agbaji\n22\nUTA\n59\n22\n0\n0\n\n\nSanti Aldama\n22\nMEM\n77\n20\n0\n1\n\n\nNickeil Alexander-Walker\n24\nMIN\n59\n3\n0\n0\n\n\n\n\n\n\nPut solution here",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#exercise-2",
    "href": "basketball/nba-ordinal-regression/index.html#exercise-2",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "EXERCISE 2",
    "text": "EXERCISE 2\nWe realize that we need to account for position. We need to do this because player position is a likely confounding variable that affects performance as players on the court have distinct responsibilities. Thus, we can also see how the position of the player influences salary bin.\nIn the chunk below, transform the Position column so that each position is now represented numerically (1 through 5, associated with the proper position: 1 is PG, 2 is SG, 3 is SF, 4 is PF, and 5 is C). Pay close attention to how the data is currently represented. A proper implementation will have a single variable called “Position”.\n\n# Your Code Here\n\n#assigns positional values for each position. Specifically,\n#1 for Point Guard, 2 for Shooting Guard, 3 for small\n#forward, 4 for power forward, and 5 for center.\nbball_clean$Position &lt;- case_when(\n  bball_clean$PG == 1 ~ 1,\n  bball_clean$SG == 1 ~ 2,\n  bball_clean$SF == 1 ~ 3,\n  bball_clean$PF == 1 ~ 4,\n  bball_clean$C == 1 ~ 5,\n  TRUE ~ 0\n)\n\nWhich position in basketball (PG, SG, SF, PF, or C) do you think gets paid the most? Why?\n\nPut solution here\nWe must now filter the Basketball Clean data frame so that it only includes players who are 25 and older. This is important because the rookie salary is calibrated differently and most rookies are under the age of 25.\nKeep in mind that filtering for ages greater than 25 decreases the number of observations in the data set. Before you do this, you need to make sure you still have enough data to create a good model.\n\nbball_clean &lt;- bball_clean %&gt;% \n  filter(Age &gt;= 25)\n\nprint(nrow(bball_clean))\n\n[1] 675\n\n\nBased on our intuition, 675 is still enough observations to conduct a sound analysis. However, to determine the lowest number of observations one can have in a dataset and still conduct ordinal logistic regression would depend on a power analysis. While power analysis is outside of the scope of this module, it is essentially a minimum sample size requirement test based on the desired level of statistical power that one seeks in conducting their statistical model. We thus conclude that 675 is enough data to create a good Ordinal Logistic Regression model.\nLet’s check our work and now visualize the data!",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#exercise-3",
    "href": "basketball/nba-ordinal-regression/index.html#exercise-3",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "EXERCISE 3",
    "text": "EXERCISE 3\nUsing the data frame, create the visualizations as instructed.\nFirst, create a barplot showing how many players are considered to be low, medium, or high.\n\n# Your Code Here\nsal_bin_counts &lt;- table(bball_clean$sal_bin)\n\nbarplot(sal_bin_counts,\n        main = \"Counts of Salary Bins\",\n        xlab = \"Salary Bin\",\n        ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nNext, let’s take a look at the distribution of NBA salaries to see if we have normality. Create a histogram representing the distribution of salaries in the NBA data. Also, determine the maximum salary of any player in the dataset.\n\n# Your Code Here\nggplot(aes(x = `Salary S`), data = bball_clean) +\n  geom_histogram(fill = 'gold') +\n  theme_classic() +\n  labs(title = \"Distribution of NBA Salaries\",\n       x = \"Salary\", y =\"Number of Players\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nhighest_value &lt;- max(bball_clean$`Salary S`)\nhighest_value\n\n[1] 51.91562\n\n\nNotice that there is a skew in the salaries, with a majority of players having a salary of approximately 2-6 million while other players have salaries upwards of 50 million. As you see, the highest salary in the dataset is 51.92 million. WOW! We need to take these specific findings into account when we later consider the validity conditions of our ordinal logistic regression model.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#exercise-4",
    "href": "basketball/nba-ordinal-regression/index.html#exercise-4",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "EXERCISE 4",
    "text": "EXERCISE 4\nWe are now curious about looking further at the relationship between Field Goals made per game versus salary. This is something we will examine below.\nCreate a scatter plot showing the relationship between points per game and salary. Add a line to it using geom_smooth(). Also, find out the maximum points per game of all players in the dataset.\n\n# Your Code Here\nggplot(data = bball_clean, aes(x = PTS_per_game, y = `Salary S`)) +\n  geom_point() +\n  geom_smooth() + \n  labs(x = \"Points per Game\", y = \"Salary\") +\n  ggtitle(\"Scatterplot of Points per Game vs. Salary\") +\n  theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nhighest_value &lt;- max(bball_clean$PTS_per_game)\nhighest_value\n\n[1] 33.1\n\n\n\nPut solution here\nBased on the visualization above, what do you think geom_smooth() does?\n\n\nPut solution here\nNow, for the final bit of data cleaning.\nWe now must turn the “low”, “medium”, and “high” values into factors (1,2,3) as this numerical encoding is needed for Ordinal Logistic Regression to work.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#exercise-5",
    "href": "basketball/nba-ordinal-regression/index.html#exercise-5",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "EXERCISE 5",
    "text": "EXERCISE 5\n\n# Turn the response (sal_bin) into a factor (1, 2, 3)\nbball_clean$sal_bin &lt;- factor(bball_clean$sal_bin, \n                              levels = c(\"Low\", \"Medium\", \"High\"), labels = c(1, 2, 3))\n\nLet’s Recap: so far, you’ve cleaned the data and visualized it. But aren’t we doing Ordinal Logistic Regression? Of course! Below is your opportunity to fit an OLR model to the data you’ve cleaned and visualized. Let’s get started.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#what-is-ordinal-logistic-regression",
    "href": "basketball/nba-ordinal-regression/index.html#what-is-ordinal-logistic-regression",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "What Is Ordinal Logistic Regression?",
    "text": "What Is Ordinal Logistic Regression?\nFirst, let’s get a deeper understanding of OLR and point you to some resources that might make it make a bit more sense. We will first provide two sources that help us to better understand OLR. We recommend that you access these sources if you seek to do a “deeper dive” into learning the theoretical aspects of OLR.\nCornell University Statistical Consulting Unit:\nJust as we can do in logistic regression, ordinal logistic regression (OLR) allows us to make predicted probabilities with just a slightly altered formula for doing so. This source shows readers how to interpret OLR coefficients which is often described as “cumbersome.”\nBy understanding how to interpret the coefficients as well as model predicted probabilities of an OLR model, we help students gain insight into a not-often taught technique. The factors that influence the ordinal response variable (salary bin) may also be able to get students motivated to view the NBA from a statistically-driven mindset.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#real-world-application-of-ordinal-logistic-regression",
    "href": "basketball/nba-ordinal-regression/index.html#real-world-application-of-ordinal-logistic-regression",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Real World Application of Ordinal Logistic Regression",
    "text": "Real World Application of Ordinal Logistic Regression\nUCLA Statistical Methods and Data Analytics:\nOLR is easily represented in real-world examples. For example, what factors contribute to whether someone orders a small, medium, or large french fry from the drive-thru? In many situations, people picture their decision-making processes ordinally. The biggest take-away from this source is how massive the application of OLR really is.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#mathematical-formulation",
    "href": "basketball/nba-ordinal-regression/index.html#mathematical-formulation",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Mathematical Formulation",
    "text": "Mathematical Formulation\nThe equation for Ordinal Logistic Regression is defined as:\n\\[ \\text{logit}(P(Y \\leq j)) = \\alpha_j + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p \\] Let’s break down the model above in its individual components.\n\n( Y ): The dependent variable, representing the ordinal response variable. This is specifically the salary bin of NBA players.\n( P(Y j) ): The probability of the dependent variable ( Y ) being less than or equal to category ( j ).\n( (P(Y j)) ): The log odds of the dependent variable ( Y ) being less than or equal to category ( j ).\n( _j ): The intercept parameter associated with category ( j ).\n( _1, _2, , _p ): The coefficients associated with the independent variables ( x_1, x_2, , x_p ) respectively.\n( x_1, x_2, , x_p ): The independent variables, representing the characteristics or attributes that influence the ordinal response variable ( Y ). This includes player stats we are looking to use in order to “predict” the salary bin.\n\nThus, the odds of being less than or equal a particular category is defined as:\n\\[ \\frac{P(Y \\leq j)}{P(Y &gt; j)}\\]\nBelow is another mathematical definition of OLR.\n\\[ \\log \\left( \\frac{P(Y \\leq j)}{P(Y &gt; j)} \\right) = logit(P(Y \\leq j)\\]",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#assumptions",
    "href": "basketball/nba-ordinal-regression/index.html#assumptions",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Assumptions",
    "text": "Assumptions\nThere are several key assumptions in Ordinal Logistic Regression. The first is the proportional odds assumption, which states that the relationship between each pair of outcome categories is consistent across all levels of the independent variables. Put more simply, the odds of being in a higher category versus a lower category must be constant across different levels of the independent variables. For example, this assumption would be violated if most players had a lower salary (which we see is somewhat true but does not make a major impact on our analysis here). Second, our observations must be independent of one another. This could potentially be violated as someone’s team might have more money and certain groups of players (think teams) could potentially be clustered and dependent on one another. To combat this in a more advanced module, one might look into conducting clustered ordinal logistic regression to account for this potentially dependent structure of the data. Next, linearity of logit. OLR assumes that the relationship between independent variables and the log odds of the outcome categories is linear.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#limitations",
    "href": "basketball/nba-ordinal-regression/index.html#limitations",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Limitations",
    "text": "Limitations\nThere are a few limitations of OLR worth mentioning.Firstly, it can be hard to interpret the coefficients from our model. For example, understanding how each predictor influences salary categories can be challenging. Thus, analysts or even team managers might not be able to use this model to make acionable decisions and should thus stick to other methods. Our model also assumes a linear relationship between predictors (player stats) and salary categories. However, this is not always a linear relationship which can thus lead to predictions of the model not being as accurate. Also, given the nature of the multiple categories we are trying to predict, OLR needs a lot of data to make accurate guesses about how player statistics relate to salaries. Thus, OLR models can only be used on larger datasets like the one we have here, which may not always be available. Finally, we acknowledge that our independence assumption is likely violated as different teams have less (or more) money to pay their players. Thus, the dependency among players within the same team violates the independence assumption and could impact our model’s estimates. One option for solving this is conducting clustered Ordinal Logistic Regression, which thus accounts for the dependence of observations between clusters (teams here). This extends beyond the scope of our module but is worthy of investigation.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#why-ordinal-logistic-regression",
    "href": "basketball/nba-ordinal-regression/index.html#why-ordinal-logistic-regression",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Why Ordinal Logistic Regression?",
    "text": "Why Ordinal Logistic Regression?\nWhy use OLR in the first place? You may have heard that Logistic Regression (LR) is one of the most popular models in statistics because of its use of a binary response variable. What might be an advantage of OLR compared to simple LR? What might be something that we need to be careful of in constructing our model (Hint, think number of bins)?\n\nPut solution here",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#fitting-the-ordinal-logistic-regression",
    "href": "basketball/nba-ordinal-regression/index.html#fitting-the-ordinal-logistic-regression",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Fitting the Ordinal Logistic Regression",
    "text": "Fitting the Ordinal Logistic Regression\nNow, it’s time to fit the model.\nBelow, we will fit an ordinal logistic regression model (m) which predicts salary bins based on various basketball player statistics and demographic factors we seek to account for. These include games, player, starts, field goals per game, three pointers per game, steals per game, blocks per game, age, total points per game, total points, and player position. We can also see the results of the fitted model below using the summary() command.\n\nm &lt;- polr(sal_bin ~ G + GS + FG_per_game + `3P_per_game` + STL_per_game +\n            BLK_per_game + Age + \n            PTS_per_game + PTS_totals + Position, data = bball_clean, Hess=TRUE)\n\nsummary(m)\n\nCall:\npolr(formula = sal_bin ~ G + GS + FG_per_game + `3P_per_game` + \n    STL_per_game + BLK_per_game + Age + PTS_per_game + PTS_totals + \n    Position, data = bball_clean, Hess = TRUE)\n\nCoefficients:\n                   Value Std. Error t value\nG             -0.0063442   0.010756 -0.5898\nGS             0.0209252   0.005689  3.6779\nFG_per_game   -0.3041243   0.466109 -0.6525\n`3P_per_game`  0.4171175   0.226359  1.8427\nSTL_per_game   0.8203322   0.335475  2.4453\nBLK_per_game   0.6404837   0.359232  1.7829\nAge            0.1601774   0.029646  5.4030\nPTS_per_game   0.3767512   0.203729  1.8493\nPTS_totals     0.0009817   0.001298  0.7566\nPosition       0.1478555   0.088574  1.6693\n\nIntercepts:\n    Value   Std. Error t value\n1|2  7.5413  1.0822     6.9684\n2|3 10.2370  1.1335     9.0309\n\nResidual Deviance: 831.2854 \nAIC: 855.2854 \n\n\nWe see that games started and age have significant positive associations with higher salaries per game, while variables like field goals per game, three pointers per game, steals per game, and player position do not show statistically significant associations with salary bins.\nHow should you interpret the OLR model? Each coefficient seen from the summary is the effect size on the log odds of being in a higher salary category. For instance, the Age coefficient of 0.160 suggests that for players older than 25, as the age of the player increases by one year, the log odds of being in a higher salary category increases by .16, holding all other variables constant.\nIn OLR, the intercepts are where we divide the categories. Their associated values indicate the log odds of being in or above the category when all predictors are zero. The AIC, or Akaike Information Criterion, is used for determining how well a model fits the data from which it was generated. In other words, the AIC helps answer the question of whether the OLR model make sense for the basketball data, and does it work for the bins into which we’ve separated player salaries?\nWe will now calculate p-values for each coefficient in the ordinal logistic regression model which will allow us another metric for understanding statistical significance. We then will construct confidence intervals for the coefficients evaluated.\n\n# Extract the coefficients table from the summary of model 'm'\nctable &lt;- coef(summary(m))\n\n# Calculate the p-values for the coefficients based on their t-values\n# 'pnorm' is used for the normal distribution, 'lower.tail = FALSE' calculates the upper tail,\n# and multiplying by 2 performs a two-tailed test\np &lt;- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n\n# Add the p-values as a new column to the coefficients table\nctable &lt;- cbind(ctable, \"p value\" = p)\n\n\nci &lt;- confint(m)\n\nWaiting for profiling to be done...\n\n\n\n# Create a table from 'ctable' using 'kable()' for better formatting in markdown or HTML outputs\nctable %&gt;% kable()\n\n\n\n\n\nValue\nStd. Error\nt value\np value\n\n\n\n\nG\n-0.0063442\n0.0107563\n-0.5898120\n0.5553167\n\n\nGS\n0.0209252\n0.0056895\n3.6778764\n0.0002352\n\n\nFG_per_game\n-0.3041243\n0.4661087\n-0.6524752\n0.5140947\n\n\n3P_per_game\n0.4171175\n0.2263586\n1.8427287\n0.0653686\n\n\nSTL_per_game\n0.8203322\n0.3354755\n2.4452822\n0.0144739\n\n\nBLK_per_game\n0.6404837\n0.3592316\n1.7829269\n0.0745982\n\n\nAge\n0.1601774\n0.0296460\n5.4030018\n0.0000001\n\n\nPTS_per_game\n0.3767512\n0.2037294\n1.8492729\n0.0644184\n\n\nPTS_totals\n0.0009817\n0.0012976\n0.7565778\n0.4493029\n\n\nPosition\n0.1478555\n0.0885738\n1.6692910\n0.0950597\n\n\n1|2\n7.5413203\n1.0822099\n6.9684451\n0.0000000\n\n\n2|3\n10.2369674\n1.1335427\n9.0309500\n0.0000000\n\n\n\n\n# this line creates a table from 'ci' using 'kable()' for clear and formatted display\nci %&gt;% kable()\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\nG\n-0.0286733\n0.0163989\n\n\nGS\n0.0098741\n0.0320792\n\n\nFG_per_game\n-1.2274034\n0.5989024\n\n\n3P_per_game\n-0.0265410\n0.8602507\n\n\nSTL_per_game\n0.1650309\n1.4831029\n\n\nBLK_per_game\n-0.0463513\n1.3611710\n\n\nAge\n0.1025227\n0.2190911\n\n\nPTS_per_game\n-0.0138302\n0.7789011\n\n\nPTS_totals\n-0.0017483\n0.0037230\n\n\nPosition\n-0.0252025\n0.3223810\n\n\n\n\n\nWe notice that the variables ‘games started’ and ‘age’ have statistically significant effects on the ordinal response variable, as they have low p-values and their confidence intervals do not cross zero.\n\n# Coefficients and confidence intervals\ncoef_df &lt;- data.frame(\n  Variable = rownames(ci),\n  Coef = coef(m),\n  CI_low = ci[,1],\n  CI_high = ci[,2]\n)\n\n# Create forest plot\nggplot(coef_df, aes(x = Coef, y = Variable)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = CI_low, xmax = CI_high), height = 0) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Coefficients and 95% Confidence Intervals\",\n       x = \"Coefficient\",\n       y = \"Variable\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nAbove we visualize the coefficients and their 95 percent confidence intervals. We see that the variables which do not overlap with zero are significant. This graph allows for a quick understanding of the direction of impact of variables and their significance for someone who does not have an understanding of statistics.\nFinally, let’s assess our model’s accuracy. How did our model do compared to the baseline accuracy of the model?\n\n# Use the predict function to get predicted categories from the\n#model 'm' using new data 'bball_clean'. The 'type = \"class\"' argument \n#specifies that the prediction should be categorical (class labels)\npredicted_categories &lt;- predict(m, bball_clean, type = \"class\")\n\n# Create a confusion matrix comparing the predicted categories \n#to the actual categories in 'bball_clean$sal_bin'\n# 'confusionMatrix' is typically from the 'caret' \n#package, which provides detailed accuracy measures and other statistics\nconf_matrix &lt;- confusionMatrix(data = predicted_categories, reference = bball_clean$sal_bin)\n\n# Extract the 'Accuracy' element from the overall results of the confusion matrix\naccuracy &lt;- conf_matrix$overall['Accuracy']\n\n# Output the accuracy\naccuracy\n\n Accuracy \n0.7274074 \n\n\n\nPut solution here",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#exercise-6",
    "href": "basketball/nba-ordinal-regression/index.html#exercise-6",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "EXERCISE 6",
    "text": "EXERCISE 6\nLet’s see how good our model is at predicting individual player salary bins. In this exercise, we’ll give you an individual player’s name. Then, you’ll make an initial guess as to which bin they’ll fall in. This initial guess should be based on the following variables: games, games started, field goals per game, age, and points per game. Finally, we’ll compare your guess to the model’s prediction.\nFirst, we need to make a more simple model that considers only the variables we outlined above. There are several implications to doing this. First, it will make the model less viable. We recommend keeping it with as many variables as the original model that was built above. Also, it may significantly alter the odds of a player being in a particular bin.\nThis exercise is more for allowing you to build your own model, then using it for a fun exercise. Enjoy!\n\nnew_m &lt;- polr(sal_bin ~ G + GS + FG_per_game + Age + PTS_per_game,\n              data = bball_clean, Hess = TRUE)\n\n\n# Use this code to see how your prediction and the model's prediction compared to reality!\n\n# bball_clean %&gt;% filter(Player == \"Player Name\") %&gt;% \n#   dplyr::select(Player, sal_bin) %&gt;% kable()",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#learning-goals-revisited",
    "href": "basketball/nba-ordinal-regression/index.html#learning-goals-revisited",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Learning Goals, Revisited",
    "text": "Learning Goals, Revisited\nAlright folks, let’s get back into the learning goals and talk through exactly what we did in this module to address them! Our module specifically introduced Ordinal Logistic Regression to predict NBA Salary Bin with the below learning modules, recapped.\n\nUnderstand how Ordinal Logistic Regression will be applied to an NBA data set and predicting NBA salary.\n\n\nYou’ve successfully applied ordinal logistic regression to predict NBA player salaries, addressing the unique challenges and considerations of this sports context.\n\n\nUnderstand the Theory and Application of Ordinal Logistic Regression (OLR).\n\n\nThrough reading explanations, coding, and creating visualizations, you now understand the theory behind OLR and its practical application in predicting salary bins.\n\n\nPrepare data for OLR.\n\n\nYou’ve data cleaned and preprocessed data necessary to prepare the NBA data set for OLR, ensuring its suitability for analysis.\n\n\nConstruct an OLR model in R using relevant variables from an NBA data set.\n\n\nYou’ve constructed and fitted an OLR model using key variables such as games played, age, positions, and performance metrics.\n\n\nInterpret OLR model coefficients and their implications for predicting NBA salary.\n\n\nYou’ve interpreted the coefficients of the OLR model, now understanding their significance and impact on predicting salary bins.\n\n\nApply OLR to predict salary and evaluate your model’s performance.\n\n\nYou’ve evaluated the model’s performance through measures like accuracy and provided insights into its effectiveness in predicting NBA player salaries.",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "basketball/nba-ordinal-regression/index.html#wrap-up-and-future-directions",
    "href": "basketball/nba-ordinal-regression/index.html#wrap-up-and-future-directions",
    "title": "Ordinal Logistic Regression Basketball Salary Prediction",
    "section": "Wrap Up and Future Directions",
    "text": "Wrap Up and Future Directions\nTo expand on this skill and apply OLR in other areas, you might do the following:\n\nFurther Model evaluation\nTo further evaluate the model, you might look into ROC curves or precision-recall curves, thus giving us a more nuanced understanding of our model’s strengths and weaknesses.\n\n\nFuture Analysis in Other Areas\nSince we’re interested in applying this in sports, you might want to look into gaining an edge by applying OLR to your fantasy sports league to forecast player performance. This could help you to identify undervalued players and inform strategic decisions such as making trades and picking up free agents. OLR would specifically be used in creating player ranking systems, player valuation systems, and even binning the value of players in terms of season long term projections. To best illustrate how this approach could be used in another elementary (non sports) situation, consider the question, “What fast food order at McDonalds is most likely to result in the customer ordering a”small”, “medium,” or “large” drink? We could thus collect data on different types of McDonalds orders and have our dependent variable be the “bin” of the type of drink ordered. Going back to sports, we could also use Ordinal Logistic Regression to answer the question, “What offensive line configurations are more likely to result in more running yards? Thus, we could bin certain amounts of”yards gained” or places on the field that the play gets to (for example: 20,40,60) as the dependent variable whereas the independent variable includes various offensive line configurations.\n\n\nRelated Concepts and Skills\nIf one is looking to learn related concepts and skills, we would recommend investigating the following types of modules shown below:\n\nMachine Learning Algorithms\nThis includes using decision trees, random forests, vector machines, and even neural networks which can allow for creating more flexible models than in Ordinal Logistic Regression. This would be best especially for optimizing one’s fantasy sports league.\n\n\nLinear Regression\nHere, we dealt with binned data. If someone wanted to try and predict a numerical response variable, we recommend starting with learning Linear Regression if it is not already understood.\nThanks for tuning in!",
    "crumbs": [
      "Home",
      "Basketball",
      "Ordinal Logistic Regression Basketball Salary Prediction"
    ]
  },
  {
    "objectID": "obstacle_courses/West_Point_Indoor_Obstacle_Course/index.html",
    "href": "obstacle_courses/West_Point_Indoor_Obstacle_Course/index.html",
    "title": "Indoor Obstacle Course Test",
    "section": "",
    "text": "Please note that these materials have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThis module is available on the ISLE platform IOCT Module",
    "crumbs": [
      "Home",
      "Obstacle Courses",
      "Indoor Obstacle Course Test"
    ]
  },
  {
    "objectID": "obstacle_courses/West_Point_Indoor_Obstacle_Course/index.html#module",
    "href": "obstacle_courses/West_Point_Indoor_Obstacle_Course/index.html#module",
    "title": "Indoor Obstacle Course Test",
    "section": "",
    "text": "Please note that these materials have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThis module is available on the ISLE platform IOCT Module",
    "crumbs": [
      "Home",
      "Obstacle Courses",
      "Indoor Obstacle Course Test"
    ]
  },
  {
    "objectID": "soccer/soccer_expected_goals/index.html",
    "href": "soccer/soccer_expected_goals/index.html",
    "title": "Expected Goals in Soccer",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThis module is available on the ISLE platform Expected Goals in Soccer Module",
    "crumbs": [
      "Home",
      "Soccer",
      "Expected Goals in Soccer"
    ]
  },
  {
    "objectID": "soccer/soccer_expected_goals/index.html#module",
    "href": "soccer/soccer_expected_goals/index.html#module",
    "title": "Expected Goals in Soccer",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThis module is available on the ISLE platform Expected Goals in Soccer Module",
    "crumbs": [
      "Home",
      "Soccer",
      "Expected Goals in Soccer"
    ]
  }
]